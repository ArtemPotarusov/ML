{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYp0bXOFK-hP"
      },
      "source": [
        "# Машинное обучение, ФКН ВШЭ\n",
        "\n",
        "## Практическое задание 8. Метод опорных векторов и аппроксимация ядер\n",
        "\n",
        "### Общая информация\n",
        "\n",
        "Дата выдачи: 30.01.2025\n",
        "\n",
        "Мягкий дедлайн: 23:59MSK 16.02.2025\n",
        "\n",
        "Жесткий дедлайн: 23:59MSK 23.02.2025\n",
        "\n",
        "### Оценивание и штрафы\n",
        "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимальная оценка за работу (без учёта бонусов) — 10 баллов.\n",
        "\n",
        "Сдавать задание после указанного жёсткого срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
        "\n",
        "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
        "\n",
        "Использование генеративных языковых моделей разрешено только в случае явного указания на это. Необходимо прописать (в соответствующих пунктах, где использовались, либо в начале/конце работы):\n",
        "- какая языковая модель использовалась\n",
        "- какие использовались промпты и в каких частях работы\n",
        "- с какими сложностями вы столкнулись при использовании генеративных моделей, с чем они помогли больше всего\n",
        "\n",
        "Неэффективная реализация кода может негативно отразиться на оценке.\n",
        "\n",
        "### Формат сдачи\n",
        "Задания сдаются через систему anytask. Посылка должна содержать:\n",
        "* Ноутбук homework-practice-08-random-features-Username.ipynb\n",
        "\n",
        "Username — ваша фамилия и имя на латинице именно в таком порядке"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY8vT0W_K-hR"
      },
      "source": [
        "### О задании\n",
        "\n",
        "На занятиях мы подробно обсуждали метод опорных векторов (SVM). В базовой версии в нём нет чего-то особенного — мы всего лишь используем специальную функцию потерь, которая не требует устремлять отступы к бесконечности; ей достаточно, чтобы отступы были не меньше +1. Затем мы узнали, что SVM можно переписать в двойственном виде, который, позволяет заменить скалярные произведения объектов на ядра. Это будет соответствовать построению модели в новом пространстве более высокой размерности, координаты которого представляют собой нелинейные модификации исходных признаков.\n",
        "\n",
        "Ядровой SVM, к сожалению, довольно затратен по памяти (нужно хранить матрицу Грама размера $d \\times d$) и по времени (нужно решать задачу условной оптимизации с квадратичной функцией, а это не очень быстро). Мы обсуждали, что есть способы посчитать новые признаки $\\tilde \\varphi(x)$ на основе исходных так, что скалярные произведения этих новых $\\langle \\tilde \\varphi(x), \\tilde \\varphi(z) \\rangle$ приближают ядро $K(x, z)$.\n",
        "\n",
        "Мы будем исследовать аппроксимации методом Random Fourier Features (RFF, также в литературе встречается название Random Kitchen Sinks) для гауссовых ядер. Будем использовать формулы, которые немного отличаются от того, что было на лекциях (мы добавим сдвиги внутрь тригонометрических функций и будем использовать только косинусы, потому что с нужным сдвигом косинус превратится в синус):\n",
        "$$\\tilde \\varphi(x) = (\n",
        "\\cos (w_1^T x + b_1),\n",
        "\\dots,\n",
        "\\cos (w_n^T x + b_n)\n",
        "),$$\n",
        "где $w_j \\sim \\mathcal{N}(0, 1/\\sigma^2)$, $b_j \\sim U[-\\pi, \\pi]$.\n",
        "\n",
        "На новых признаках $\\tilde \\varphi(x)$ мы будем строить любую линейную модель.\n",
        "\n",
        "Можно считать, что это некоторая новая парадигма построения сложных моделей. Можно направленно искать сложные нелинейные закономерности в данных с помощью градиентного бустинга или нейронных сетей, а можно просто нагенерировать большое количество случайных нелинейных признаков и надеяться, что быстрая и простая модель (то есть линейная) сможет показать на них хорошее качество. В этом задании мы изучим, насколько работоспособна такая идея.\n",
        "\n",
        "### Алгоритм\n",
        "\n",
        "Вам потребуется реализовать следующий алгоритм:\n",
        "1. Понизить размерность выборки до new_dim с помощью метода главных компонент.\n",
        "2. Для полученной выборки оценить гиперпараметр $\\sigma^2$ с помощью эвристики (рекомендуем считать медиану не по всем парам объектов, а по случайному подмножеству из где-то миллиона пар объектов): $$\\sigma^2 = \\text{median}_{i, j = 1, \\dots, \\ell, i \\neq j} \\left\\{\\sum_{k = 1}^{d} (x_{ik} - x_{jk})^2 \\right\\}$$\n",
        "3. Сгенерировать n_features наборов весов $w_j$ и сдвигов $b_j$.\n",
        "4. Сформировать n_features новых признаков по формулам, приведённым выше.\n",
        "5. Обучить линейную модель (логистическую регрессию или SVM) на новых признаках.\n",
        "6. Повторить преобразования (PCA, формирование новых признаков) к тестовой выборке и применить модель."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ljk_pd4qaj6"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_sGunb7K-hS"
      },
      "source": [
        "Тестировать алгоритм мы будем на данных Fashion MNIST. Ниже код для их загрузки и подготовки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyG6dBfjK-hS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "# 1 Способ\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train_pics, y_train), (x_test_pics, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# 2 Способ (если первый не работает)\n",
        "# from sklearn.datasets import fetch_openml\n",
        "# def load_fashion_mnist():\n",
        "#     X, y = fetch_openml('Fashion-MNIST', version=1, return_X_y=True, as_frame=False)\n",
        "#     X = X.reshape(-1, 28, 28).astype('uint8')\n",
        "#     y = y.astype('int64')\n",
        "#     x_train, x_test = X[:60000], X[60000:]\n",
        "#     y_train, y_test = y[:60000], y[60000:]\n",
        "#     return (x_train, y_train), (x_test, y_test)\n",
        "# (x_train_pics, y_train), (x_test_pics, y_test) = load_fashion_mnist()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train = x_train_pics.reshape(y_train.shape[0], -1)\n",
        "X_test = x_test_pics.reshape(y_test.shape[0], -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNvzl7Glqaj9"
      },
      "source": [
        "__Задание 0. (0.25 баллов)__\n",
        "\n",
        "**Вопрос:** зачем в алгоритме нужен метод главных компонент?\n",
        "\n",
        "**Ответ:** Интуитивно я бы сказал по тому, что главные (сильные) признаки при объединении дадут хорошие новые признаки, а всякие полукровки с маглами вообще потеряют магию (вызовут переобучение). Потому мы сначала выдедим лучшие, а затем уже на меньшем множестве (еще и генерация быстрее будет происходить) создадим новые"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJNN55F7K-hT"
      },
      "source": [
        "__Задание 1. (3 балла)__\n",
        "\n",
        "Реализуйте алгоритм, описанный выше. Можете воспользоваться шаблоном класса в `homework_practice_08_rff.py` (допишите его и исправьте несостыковки в классе пайплайна) или написать свой интерфейс.\n",
        "\n",
        "Ваша реализация должна поддерживать следующие опции:\n",
        "1. Возможность задавать значения гиперпараметров new_dim (по умолчанию 50) и n_features (по умолчанию 1000).\n",
        "2. Возможность включать или выключать предварительное понижение размерности с помощью метода главных компонент.\n",
        "3. Возможность выбирать тип линейной модели (логистическая регрессия или SVM с линейным ядром).\n",
        "\n",
        "Протестируйте на данных Fashion MNIST, сформированных кодом выше. Если на тесте у вас получилась доля верных ответов не ниже 0.84 с гиперпараметрами по умолчанию, то вы всё сделали правильно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP8yepx8K-hT",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0f6fda-79f2-4388-91ba-2e959201bd07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8613\n"
          ]
        }
      ],
      "source": [
        "from homework_practice_08_rff import RFFPipeline, RandomFeatureCreator\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "pipeline = RFFPipeline(n_features=1000, new_dim=50, feature_creator_class=RandomFeatureCreator)\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим, алгоритм перформит."
      ],
      "metadata": {
        "id": "nnkTVBw7SfZU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYqQUEi-K-hU"
      },
      "source": [
        "__Задание 2. (2.5 балла)__\n",
        "\n",
        "Сравните подход со случайными признаками с обучением SVM на исходных признаках. Попробуйте вариант с обычным (линейным) SVM и с ядровым SVM. Ядровой SVM может очень долго обучаться, поэтому можно делать любые разумные вещи для ускорения: брать подмножество объектов из обучающей выборки, например.\n",
        "\n",
        "Сравните подход со случайными признаками с вариантом, в котором вы понижаете размерность с помощью PCA и обучите градиентный бустинг. Используйте одну из реализаций CatBoost/LightGBM/XGBoost.\n",
        "\n",
        "Сделайте выводы — насколько идея со случайными признаками работает? Сравните как с точки зрения качества, так и с точки зрения скорости обучения и применения."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, Y_train = X_train, y_train"
      ],
      "metadata": {
        "id": "DERDYeHwzXIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subsample = 7777\n",
        "idx_sub = np.random.choice(np.arange(X_train.shape[0]), size=subsample, replace=False)\n",
        "X_train = X_train[idx_sub]\n",
        "y_train = y_train[idx_sub]"
      ],
      "metadata": {
        "id": "PZVFHcZrzdj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN8LUlJgK-hV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130b3771-e144-4d0c-8c33-7425ac85b80b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM accuracy = 0.7664, time = 398.81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "start_time = time.time()\n",
        "linear_svm = LinearSVC()\n",
        "linear_svm.fit(X_train, y_train)\n",
        "y_pred_linear = linear_svm.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "print(f\"Linear SVM accuracy = {acc_linear:.4f}, time = {time.time()-start_time:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "kernel_svm = SVC(kernel='rbf', gamma='scale')\n",
        "kernel_svm.fit(X_train, y_train)\n",
        "y_pred_kernel = kernel_svm.predict(X_test)\n",
        "acc_kernel = accuracy_score(y_test, y_pred_kernel)\n",
        "print(f\"Kernel SVM accuracy = {acc_kernel:.4f}, time = {time.time()-start_time:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNp4eInSvAIt",
        "outputId": "1658321f-c0e2-4096-9568-988e1015c50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kernel SVM accuracy = 0.8487, time = 48.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = x_train, Y_train"
      ],
      "metadata": {
        "id": "hPxMjBQpD8wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "pca = PCA(n_components=50, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "xgb = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
        "xgb.fit(X_train_pca, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test_pca)\n",
        "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost: accuracy = {acc_xgb:.4f}, time = {time.time()-start_time:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJHAgGwasx-3",
        "outputId": "6e52e3d9-d6c0-4b96-e1ed-8573e9336520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost: accuracy = 0.8711, time = 38.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "С лучшими парметрами (перебирал оптуной (убрал ее потому как дохрена долго работала) accuracy 0.8858)\n",
        "\n",
        "    n_components: 98\n",
        "    max_depth: 6\n",
        "    learning_rate: 0.1913593150564032\n",
        "    subsample: 0.6269101897488234\n",
        "    colsample_bytree: 0.8380798840530911\n",
        "    n_estimators: 290"
      ],
      "metadata": {
        "id": "qBAZzgBvAjjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим, бустинг, даже без подбора параметров, показывает результат лучше, чем SVM, причем как по акураси, так и по времени. Обычный SVM работал дольше всех, так еще и качество не очень, его модификация стала работать быстрее, чем самописный rff (48 секунд против 60), но и качество немного просело (0,8487 против 0,86). В целом можно говорить о том, что бустинг работает надежнее всех (по крайней мере в такой незамысловатой задаче)."
      ],
      "metadata": {
        "id": "z84k25U5TNsw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6umjhWuK-hV"
      },
      "source": [
        "__Задание 3. (2 балла)__\n",
        "\n",
        "Проведите эксперименты:\n",
        "1. Помогает ли предварительное понижение размерности с помощью PCA?\n",
        "2. Как зависит итоговое качество от n_features? Выходит ли оно на плато при росте n_features?\n",
        "3. Важно ли, какую модель обучать — логистическую регрессию или SVM?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "start_time = time.time()\n",
        "rff_pipe = RFFPipeline(n_features=1000, new_dim=50, use_PCA=True,\n",
        "    feature_creator_class=RandomFeatureCreator)\n",
        "rff_pipe.fit(X_train, y_train)\n",
        "y_pred_rff = rff_pipe.predict(X_test)\n",
        "acc_rff = accuracy_score(y_test, y_pred_rff)\n",
        "print(f\"RFF accuracy = {acc_rff:.4f}, time = {time.time()-start_time:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ya8kfbztUgr",
        "outputId": "e3629679-6cae-4e1a-9b4f-e88a18ccafe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFF accuracy = 0.8616, time = 80.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "pipeline = RFFPipeline(n_features=1000, new_dim=50, feature_creator_class=RandomFeatureCreator)\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy = {acc_rff:.4f}, time = {time.time()-start_time:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEnGlLraCn-r",
        "outputId": "65df5ad4-102e-4f2d-d747-025780781918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.8616, time = 62.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В целом (смотря еще на дальнейшие эксперименты), я бы сказал, что PCA помогает улучшить метрики (причем очень существенно, например для Лапласа). При этом, при базовых параметрах эффект не особо виден."
      ],
      "metadata": {
        "id": "aM2Et9w9xrm6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2QIHIMbK-hW"
      },
      "outputs": [],
      "source": [
        "n_features_values = [50, 100, 200, 500, 1500]\n",
        "for n_feat in n_features_values:\n",
        "    pipeline = RFFPipeline(n_features=1000, new_dim=50, feature_creator_class=RandomFeatureCreator)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"n_features = {n_feat}: accuracy = {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим, что если использовать PCA, то при росте n_features качество растет и дейстивтельно выходит на плато при дальнейшем увеличении (результат ниже, связано это с тем, что мы из большего набора способны выбрать лучший результат).\n",
        "\n",
        "    n_features = 50: accuracy = 0.7749\n",
        "    n_features = 100: accuracy = 0.8081\n",
        "    n_features = 200: accuracy = 0.8346\n",
        "    n_features = 500: accuracy = 0.8565\n",
        "    n_features = 1500: accuracy = 0.8646\n",
        "\n",
        "Если же PCA не использовать, то прироста это не дает (результаты в таком формате, т.к. запускал я не убрав ворнинги, а перезапускать долго).\n",
        "\n",
        "    n_features = 50: accuracy = 0.8640\n",
        "    n_features = 100: accuracy = 0.8622\n",
        "    n_features = 200: accuracy = 0.8630\n",
        "    n_features = 500: accuracy = 0.8619\n",
        "    n_features = 1500: accuracy = 0.8633"
      ],
      "metadata": {
        "id": "l3eRLnJ5yNVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_lr = RFFPipeline(\n",
        "    n_features=1000,\n",
        "    new_dim=50,\n",
        "    use_PCA=True,\n",
        "    feature_creator_class=RandomFeatureCreator,\n",
        "    classifier_params={'max_iter': 1000, 'random_state': 42},\n",
        "    func=np.cos\n",
        ")\n",
        "pipe_lr.fit(X_train, y_train)\n",
        "acc_lr = accuracy_score(y_test, pipe_lr.predict(X_test))\n",
        "\n",
        "pipe_svm = RFFPipeline(\n",
        "    n_features=1000,\n",
        "    new_dim=50,\n",
        "    use_PCA=True,\n",
        "    feature_creator_class=RandomFeatureCreator,\n",
        "    classifier_class=LinearSVC,\n",
        "    classifier_params={'max_iter': 10000, 'random_state': 42},\n",
        "    func=np.cos\n",
        ")\n",
        "pipe_svm.fit(X_train, y_train)\n",
        "acc_svm = accuracy_score(y_test, pipe_svm.predict(X_test))\n",
        "\n",
        "print(f\"LogisticRegression accuracy = {acc_lr:.4f}\")\n",
        "print(f\"LinearSVC accuracy = {acc_svm:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNgQ_tdrwdNb",
        "outputId": "cfa17877-7369-4fe6-8845-155abf18c297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression accuracy = 0.8627\n",
            "LinearSVC accuracy = 0.8694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видно, что качество отличается не сильно, однако т.к. метод у нас заточен под SVM, да и нашей метрике побоку веротяности, SVM все же перформит лучше."
      ],
      "metadata": {
        "id": "NZVtBEv82Opu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVDWHCdrK-hX"
      },
      "source": [
        "__Задание 4. (Максимум 1.5 балла)__\n",
        "\n",
        "Как вы, должно быть, помните с курса МО-1, многие алгоритмы машинного обучения работают лучше, если признаки данных некоррелированы. Оказывается, что для RFF существует модификация, позволяющая получать ортогональные случайные признаки (Orthogonal Random Features, ORF). Об этом методе можно прочитать в [статье](https://proceedings.neurips.cc/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf). Реализуйте класс для вычисления ORF по аналогии с основным заданием. Обратите внимание, что ваш класс должен уметь работать со случаем n_features > new_dim (в статье есть замечание на этот счет), n_features=new_dim и n_features < new_dim также должны работать, убедитесь в этом. Проведите эксперименты, сравнивающие RFF и ORF, сделайте выводы.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSxvGI9iK-hX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8da8f70-cd32-47e9-c573-5365a28bbfd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFF accuracy = 0.8635, time = 65.10\n"
          ]
        }
      ],
      "source": [
        "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "from homework_practice_08_rff import OrthogonalRandomFeatureCreator\n",
        "start_time = time.time()\n",
        "rff_pipe = RFFPipeline(n_features=1000, new_dim=50, use_PCA=True,\n",
        "    feature_creator_class=OrthogonalRandomFeatureCreator)\n",
        "rff_pipe.fit(X_train, y_train)\n",
        "y_pred_rff = rff_pipe.predict(X_test)\n",
        "acc_rff = accuracy_score(y_test, y_pred_rff)\n",
        "print(f\"RFF accuracy = {acc_rff:.4f}, time = {time.time()-start_time:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим, ORF работает чуть лучше, чем RFF (если поиграть с параметрами, можно еще пару пунктов выжать). По времени работает немного быстрее, да и видимо более численно устойчив (из-за ортогонализации)."
      ],
      "metadata": {
        "id": "xuUV8u7IXbys"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pc7-1jmK-hY"
      },
      "source": [
        "__Задание 5. (Максимум 1 балл)__\n",
        "\n",
        "Существует большое количество работ, где идея RFF развивается, предлагаются её обобщения (которые, по сути, выливаются в другие преобразования признаков, не обязательно уже тригонометрические). Возьмите любую из таких работ, кратко опишите идею, имплементируйте её и сравните качество с ORF и RFF, которые вы запрограммировали выше.\n",
        "\n",
        "Ссылки на статьи, где обсуждаются вариации RFF для разных ядер, можно найти в окрестности таблицы 1 в работе https://arxiv.org/pdf/1407.5599  \n",
        "\n",
        "___ссылка на работу:___ https://chbrown.github.io/kdd-2013-usb/kdd/p239.pdf\n",
        "\n",
        "___описание идеи:___ Мы решаем аппроксимировать ядро $(<x,y> + c)^p$ через быстрое вычисление Count Sketch (по-сути это свертка вектора используя хэш-функции на индексы и ±1) а также fft для быстрого подсчета."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я пробовал еще QMC, Лапласа и Коши. QMC показал неплохой результат, Лаплас работал плохо, Коши работал хорошо"
      ],
      "metadata": {
        "id": "IwRgURF3ZKX4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWj-O2vjK-hY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4a5fe8-0375-4017-c604-769b46b3e27b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFF accuracy = 0.8604, time = 201.14\n"
          ]
        }
      ],
      "source": [
        "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "from homework_practice_08_rff import TensorSketchFeatureCreator\n",
        "start_time = time.time()\n",
        "rff_pipe = RFFPipeline(n_features=1000, new_dim=50, use_PCA=True,\n",
        "    feature_creator_class=TensorSketchFeatureCreator)\n",
        "rff_pipe.fit(X_train, y_train)\n",
        "y_pred_rff = rff_pipe.predict(X_test)\n",
        "acc_rff = accuracy_score(y_test, y_pred_rff)\n",
        "print(f\"RFF accuracy = {acc_rff:.4f}, time = {time.time()-start_time:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим, что метод стал работаь медленнее, однако качество остается достаточно высоким и сравнимы как с RFF, так и с ORF."
      ],
      "metadata": {
        "id": "r4RqPPQPXuNK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJvyGXqZqakC"
      },
      "source": [
        "__Задание 6. (Максимум 2.5 балла)__\n",
        "\n",
        "Реализуйте класс ядровой Ridge регрессии (Лекция 13, $\\S 1.2$), для оптимизации используте градиентный спуск **[1 балл максимум]**, также добавьте возможность использовать аналитическую формулу **[1 балл максимум]**. Для градиентного спуска выпишите градиент ниже **[0.5 баллов максимум]**.\n",
        "Подумайте о том, как в формулах правильно учесть свободный коэффициент.\n",
        "\n",
        "Затем адаптируйте вашу реализацию RFF под задачу регрессии. Сравните вашу ядровую регрессию и RFF на синтетических данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g2W_BHdqakC"
      },
      "source": [
        "Функция потерь:\n",
        "$$\n",
        "Q(w) = \\frac{1}{2} ||\\Phi \\Phi^T w - y||^2 + \\frac{\\lambda}{2} w^T \\Phi \\Phi^T w \\rightarrow \\min_w,\n",
        "$$\n",
        "где $\\Phi \\Phi^T = K$, $K = (k(x_i, x_j))_{i, j = 1}^{\\ell}$.\n",
        "\n",
        "Предсказание:\n",
        "$\n",
        "y(x) = k(x)^T w,\n",
        "$\n",
        "где $k(x)$ — вектор функций ядра от пар объектов $(x, x_i)_{i=1}^{\\ell}$.\n",
        "\n",
        "___Выведите градиент:___\n",
        "$$\n",
        "\\nabla Q(w) \\implies d\\frac{(y^t - w^tK)(Kw - y) + λw^tKw}{2} = \\frac{dw^t(λKw - K(Kw-y)) + ((y^t - w^tK)K + λw^tK)dw}{2} = |стоит\\ скаляр\\ потому\\ транспонируем| = (K(Kw-y) + λKw)^tdw \\implies \\nabla Q(w) = K(Kw- y + λw)\n",
        "$$\n",
        "\n",
        "Вы можете изменять представленный шаблон в файле `homework_practice_08_kernel_regression.py` по своему усмотрению."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "X, y = make_regression(n_samples=3000, n_features=100, noise=0.1)\n",
        "ss = StandardScaler()\n",
        "X = ss.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "metadata": {
        "id": "W6htMdBFbC_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYKi5mXWqakC",
        "outputId": "dcaf0675-633a-4ed8-cdc2-be5b2a4be128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE = 22812.5675, time = 0.67\n"
          ]
        }
      ],
      "source": [
        "from homework_practice_08_kernel_regression import KernelRidgeRegression\n",
        "\n",
        "start_time = time.time()\n",
        "kernel_linreg = KernelRidgeRegression().fit_closed_form(X_train, y_train)\n",
        "y_pred_reg = kernel_linreg.predict(X_test)\n",
        "acc = mse(y_test, y_pred_reg)\n",
        "print(f\"MSE = {acc:.4f}, time = {time.time()-start_time:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from homework_practice_08_rff import RFFPipeline, RandomFeatureCreator\n",
        "\n",
        "pipeline = RFFPipeline(\n",
        "    n_features=250,\n",
        "    feature_creator_class=RandomFeatureCreator,\n",
        "    regression=True\n",
        ")\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "accuracy = mse(y_test, y_pred)\n",
        "print(\"MSE:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjv40l95pcCR",
        "outputId": "1736fc6c-c7c6-45cb-d34f-aa01930d6deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 3016.3979959673647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ну я думаю тут все и без слов понятно... Возможно, надо какие-то оболее адекватные данные, но мы видим, что даже без PCA RFF показывает существенно лучший резульат."
      ],
      "metadata": {
        "id": "6FHrm_YBfdxT"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}